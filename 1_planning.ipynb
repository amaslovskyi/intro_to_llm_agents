{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install -U langchain-community tavily-python\n",
    "%pip install -uq langchain-openai tavily-python\n",
    "%pip install beautifulsoup4\n",
    "%pip install faiss-cpu\n",
    "%pip install langchainhub\n",
    "%pip install -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import yaml\n",
    "\n",
    "# config = yaml.safe_load(open(\"myconfig.yml\"))\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = config[\"OPEN_AI_API_KEY\"]\n",
    "# os.environ[\"TAVILY_API_KEY\"] = config[\"TAVILY_API_KEY\"]\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "# os.environ[\"LANGCHAIN_HUB_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = str(config[\"LANGCHAIN_TRACING_V2\"]).lower()\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = config[\"LANGCHAIN_ENDPOINT\"]\n",
    "# os.environ[\"LANGCHAIN_HUB_API_URL\"] = config[\"LANGCHAIN_HUB_API_URL\"]\n",
    "# os.environ[\"LANGCHAIN_WANDB_TRACING\"] = str(config[\"LANGCHAIN_WANDB_TRACING\"]).lower()\n",
    "# os.environ[\"WANDB_PROJECT\"] = config[\"WANDB_PROJECT\"]\n",
    "\n",
    "# Import and load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "load_dotenv()\n",
    "#Python library used to display progress bars for loops\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "\n",
    "![images/Screenshot 2024-02-21 at 16.17.22.png](<images/Screenshot 2024-02-21 at 16.17.22.png>)\n",
    "\n",
    "You might have come across various techniques aimed at improving the performance of large language models, such as offering tips or even jokingly threatening them. One popular technique is called \"chain of thought,\" where the model is asked to think step by step, enabling self-correction. This approach has evolved into more advanced versions like \"chain of thought with self-consistency\" and the generalized \"tree of thoughts,\" where multiple thoughts are created, re-evaluated, and consolidated to provide an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree of Thoughts\n",
    "\n",
    "The [@astropomeai tutorial](https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac) on Tree of Thoughts is used as basis of this exercise but expanded with LLMOps tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "/Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "/Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "/Users/Shatten/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "cot_step1 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step1\")\n",
    "cot_step2 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step2\")\n",
    "cot_step3 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step3\")\n",
    "cot_step4 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/5skqq8p925lbtt_m8d7msnkc0000gn/T/ipykernel_87577/3545555269.py:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain1 = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "# model = \"gpt-3.5-turbo\"\n",
    "model = OllamaLLM(model=\"llama3.2-vision\")\n",
    "\n",
    "# chain1 = LLMChain(\n",
    "#     llm=ChatOpenAI(temperature=0, model=model),\n",
    "#     prompt=cot_step1,\n",
    "#     output_key=\"solutions\"\n",
    "# )\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOllama(temperature=0, model=\"llama3.2-vision\"),\n",
    "    prompt=cot_step1,\n",
    "    output_key=\"solutions\",\n",
    "    # other params ...\n",
    ")\n",
    "\n",
    "# chain2 = LLMChain(\n",
    "#     llm=ChatOpenAI(temperature=0, model=model),\n",
    "#     prompt=cot_step2,\n",
    "#     output_key=\"review\"\n",
    "# )\n",
    "\n",
    "chain2 = LLMChain( \n",
    "    llm = ChatOllama(temperature=0, model=\"llama3.2-vision\"),\n",
    "    prompt=cot_step2,\n",
    "    output_key=\"review\",\n",
    ")\n",
    "\n",
    "# chain3 = LLMChain(\n",
    "#     llm=ChatOpenAI(temperature=0, model=model),\n",
    "#     prompt=cot_step3,\n",
    "#     output_key=\"deepen_thought_process\"\n",
    "# )\n",
    "\n",
    "chain3 = LLMChain( \n",
    "    llm = ChatOllama(temperature=0, model=\"llama3.2-vision\"),\n",
    "    prompt=cot_step3,\n",
    "    output_key=\"deepen_thought_process\",\n",
    ")\n",
    "\n",
    "# chain4 = LLMChain(\n",
    "#     llm=ChatOpenAI(temperature=0, model=model),\n",
    "#     prompt=cot_step4,\n",
    "#     output_key=\"ranked_solutions\"\n",
    "# )\n",
    "\n",
    "chain4 = LLMChain( \n",
    "    llm = ChatOllama(temperature=0, model=\"llama3.2-vision\"),\n",
    "    prompt=cot_step4,\n",
    "    output_key=\"ranked_solutions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/5skqq8p925lbtt_m8d7msnkc0000gn/T/ipykernel_87577/3364929189.py:11: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  overall_chain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'human colonization of Mars', 'perfect_factors': 'The distance between Earth and Mars is very large, making regular resupply difficult', 'ranked_solutions': \"Your assessment of Solution 1: In-Situ Resource Utilization (ISRU) seems reasonable. Here's a breakdown of your evaluation:\\n\\n**Strengths:**\\n\\n* Reduced reliance on Earth-based supplies\\n* Increased self-sufficiency\\n* Potential for in-situ manufacturing\\n\\nThese benefits are significant, as they can help reduce the logistical challenges and costs associated with establishing a human settlement on Mars.\\n\\n**Weaknesses:**\\n\\n* Technological hurdles\\n* Resource availability\\n* Environmental concerns\\n\\nThese challenges are indeed significant, but it's not impossible to overcome them. The strategies you proposed for addressing these challenges (robust R&D, collaboration with experts, monitoring and adaptation) are sound and can help mitigate the risks.\\n\\n**Probability of Success:** 6/10\\n**Confidence Level:** 7/10\\n\\nI agree that a probability of success rating of 6/10 is reasonable. While ISRU has many potential benefits, it's still an untested approach in the context of human colonization on Mars. There are many unknowns and uncertainties that need to be addressed before ISRU can be considered a viable solution.\\n\\nThe confidence level of 7/10 suggests that you have some faith in the feasibility of ISRU, but there are still significant risks and uncertainties involved. This is a reasonable assessment, given the complexity of the challenges involved.\\n\\nNow, let's move on to Solution 2: Terraforming!\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\n",
    "        overall_chain(\n",
    "            {\n",
    "                \"input\": \"human colonization of Mars\",\n",
    "                \"perfect_factors\": \"The distance between Earth and Mars is very large, making regular resupply difficult\"\n",
    "            }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct prompt overview\n",
    "\n",
    "Let's review [ReAct](https://python.langchain.com/docs/modules/agents/agent_types/react) prompt as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-ask with search\n",
    "\n",
    "Let's review [self-ask with search](https://python.langchain.com/docs/modules/agents/agent_types/self_ask_with_search) as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Who lived longer, Muhammad Ali or Alan Turing?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\n\\nQuestion: When was the founder of craigslist born?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the founder of craigslist?\\nIntermediate answer: Craigslist was founded by Craig Newmark.\\nFollow up: When was Craig Newmark born?\\nIntermediate answer: Craig Newmark was born on December 6, 1952.\\nSo the final answer is: December 6, 1952\\n\\nQuestion: Who was the maternal grandfather of George Washington?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball\\n\\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?\\nAre follow up questions needed here: Yes.\\nFollow up: Who is the director of Jaws?\\nIntermediate answer: The director of Jaws is Steven Spielberg.\\nFollow up: Where is Steven Spielberg from?\\nIntermediate answer: The United States.\\nFollow up: Who is the director of Casino Royale?\\nIntermediate answer: The director of Casino Royale is Martin Campbell.\\nFollow up: Where is Martin Campbell from?\\nIntermediate answer: New Zealand.\\nSo the final answer is: No\\n\\nQuestion: {input}\\nAre followup questions needed here:{agent_scratchpad}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches to read next:\n",
    "\n",
    "**Reflexion** ([Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills.\n",
    "\n",
    "**Chain of Hindsight** (CoH; [Liu et al. 2023](https://arxiv.org/abs/2302.02676)) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
